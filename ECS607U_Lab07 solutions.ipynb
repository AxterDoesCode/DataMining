{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIbkT8a0f3qQ"
      },
      "source": [
        "# Lab session 7: Outlier Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu-2ZNSGf3qS"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab (Lab session 7) is for students to get experience with **Outlier Detection** covered in week 9, by using typical Python libraries.\n",
        "\n",
        "\n",
        "This session starts with a tutorial that uses examples to introduce you to the practical knowledge that you will need for the corresponding assignment. We highly recommend that you read the following tutorials if you need a gentler introduction to the libraries that we use:\n",
        "- [Numpy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html)\n",
        "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
        "- [Matplotlib](https://matplotlib.org/tutorials/introductory/pyplot.html)\n",
        "- [Scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWRYEF6ff3qX"
      },
      "source": [
        "## 1. Outlier detection using parametric methods \n",
        "\n",
        "This approach assumes that the majority of the data instances are governed by some well-known probability distribution, e.g. a Gaussian distribution. Outliers can then be detected by seeking for observations that do not fit the overall distribution of the data. \n",
        "\n",
        "In this example, our goal is to detect anomalous changes in the daily closing prices of various stocks. The input data **stocks.csv** (available in the lab supplementary material) contains the historical closing prices of stocks for 3 large corporations (Microsoft, Ford Motor Company, and Bank of America). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW0ujwgRf3qZ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV file, set the 'Date' values as the index of each row, and display the first rows of the dataframe\n",
        "stocks = pd.read_csv('stocks.csv', header='infer') \n",
        "stocks.index = stocks['Date']\n",
        "stocks = stocks.drop(['Date'],axis=1)\n",
        "stocks.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n3kJ7PIf3qZ"
      },
      "source": [
        "We can compute the percentage of changes in the daily closing price of each stock as follows:\n",
        "\\begin{equation}\n",
        "\\Delta(t) = 100 \\times \\frac{x_t - x_{t-1}}{x_{t-1}} \n",
        "\\end{equation}\n",
        "\n",
        "where $x_t$ denotes the price of a stock on day $t$ and $x_{t-1}$ denotes the price on its previous day, $t-1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X9G1gl5f3qa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "N,d = stocks.shape\n",
        "# Compute delta, which denotes the percentage of changes in the daily closing price of each stock\n",
        "delta = pd.DataFrame(100*np.divide(stocks.iloc[1:,:].values-stocks.iloc[:N-1,:].values, stocks.iloc[:N-1,:].values),\n",
        "                    columns=stocks.columns, index=stocks.iloc[1:].index)\n",
        "delta.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnI5-IUwf3qb"
      },
      "source": [
        "We can now plot the distribution of the percentage daily changes in stock price as a 3-dimensional scatter plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tE6I65yf3qb"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(8,5)).gca(projection='3d')\n",
        "fig.scatter(delta.MSFT,delta.F,delta.BAC)\n",
        "fig.set_xlabel('Microsoft')\n",
        "fig.set_ylabel('Ford')\n",
        "fig.set_zlabel('Bank of America')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_99E3HpUf3qd"
      },
      "source": [
        "Let's assume that the data follows a multivariate (i.e. multidimensional) Gaussian distribution. Such a probability distribution can be characterised by two statistics: the mean and covariance matrix of the 3-dimensional data. \n",
        "\n",
        "We can then compute the mean and covariance matrix of the 3-dimensional 'delta' data (which represent the percentage of changes in the daily closing price of each stock). Then, as a distance measure, to determine the anomalous trading days, we can compute the **Mahalanobis distance** (to be more precise, the square of the Mahalanobis distance) between the percentage of price change on each day against the mean percentage of price change:\n",
        "\\begin{equation}\n",
        "\\textrm{MDist}(x,\\bar{x}) = (x - \\bar{x})^T S^{-1}(x - \\bar{x})\n",
        "\\end{equation}\n",
        "where $x$ is assumed to be a row vector, $\\bar{x}$ denotes the mean vector, and $S$ denotes the covariance matrix of the data.\n",
        "\n",
        "See Section 12.3 in the \"Data Mining: Concepts and Techniques\" book for more information on the Mahalanobis distance. As a first step, we can define a function for the Mahalanobis distance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ek3yMFf3qd"
      },
      "source": [
        "from numpy.linalg import inv\n",
        "\n",
        "def mahalanobis(x=None, data=None):\n",
        "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
        "    x    : vector or matrix of data with, say, p columns.\n",
        "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
        "    \"\"\"\n",
        "    x_mu = x - np.mean(data)\n",
        "    cov = np.cov(data.values.T)\n",
        "    inv_covmat = np.linalg.inv(cov)\n",
        "    left = np.matmul(x_mu, inv_covmat)\n",
        "    mahal = np.dot(left, x_mu.T)\n",
        "    return mahal.diagonal()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2gCPk6of3qe"
      },
      "source": [
        "Then, we can call the created function for the Mahalanobis distance on the 'delta' dataframe containing the daily percentage changes for each stock:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl6aSECAf3qf"
      },
      "source": [
        "# Compute Mahalanobis distance for delta dataset\n",
        "mahal = mahalanobis(x=delta, data=delta[['MSFT', 'F', 'BAC']])\n",
        "\n",
        "# Assign an outlier score for the data based on the computed Mahalanobis distance\n",
        "outlier_score = mahal\n",
        "\n",
        "# Display 3D scatterplot with datapoints having a different color according to their outlier score\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "p = ax.scatter(delta.MSFT,delta.F,delta.BAC,c=outlier_score,cmap='jet')\n",
        "ax.set_xlabel('Microsoft')\n",
        "ax.set_ylabel('Ford')\n",
        "ax.set_zlabel('Bank of America')\n",
        "fig.colorbar(p)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEhzihaAf3qf"
      },
      "source": [
        "The top outliers are shown as the dark red and orange points in the above scatterplot. We can examine the dates associated with the top-5 highest outlier scores as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93dPy4mLf3qf"
      },
      "source": [
        "outlier = pd.DataFrame(outlier_score, index=delta.index, columns=['Outlier score'])\n",
        "result = pd.concat((delta,outlier), axis=1)\n",
        "result.nlargest(5,'Outlier score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFYelx7Kf3qg"
      },
      "source": [
        "We see for example that the top outlier was detected for 13th October 2008. This was a period of economic instability due to the 2008 global financial crisis (https://en.wikipedia.org/wiki/Global_financial_crisis_in_October_2008).\n",
        "\n",
        "We can subsequently inspect stocks around the top-2 outlier dates for each company, and see to which compani(es) are these outliers attributed to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ITrKNVmjf3qg"
      },
      "source": [
        "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
        "\n",
        "ts = delta[445:452]\n",
        "ts.plot.line(ax=ax1)\n",
        "ax1.set_xticks(range(7))\n",
        "ax1.set_xticklabels(ts.index)\n",
        "ax1.set_ylabel('Percent Change')\n",
        "\n",
        "ts = delta[477:484]\n",
        "ts.plot.line(ax=ax2)\n",
        "ax2.set_xticks(range(7))\n",
        "ax2.set_xticklabels(ts.index)\n",
        "ax2.set_ylabel('Percent Change')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PpbBA0hf3qh"
      },
      "source": [
        "## 2. Outlier detection using proximity-based approaches\n",
        "\n",
        "This is a model-free outlier detection approach as it does not require constructing an explicit model of the normal class to determine the outlier score of data instances. The example code shown below employs the k-nearest neighbor approach to calculate the outlier score. Specifically, a normal instance is expected to have a small distance to its k-th nearest neighbour whereas an outlier is likely to have a large distance to its k-th nearest neighbour. In the example below, we apply the distance-based approach with k=4 to identify the anomalous trading days from the stock market data described in the previous section.\n",
        "\n",
        "For more information on the NearestNeighbors() function please see the scikit learn documnetation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id7jJhLtf3qh"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Implement a k-nearest neighbour approach using k=4 neighbours\n",
        "knn = 4\n",
        "nbrs = NearestNeighbors(n_neighbors=knn, metric=distance.euclidean).fit(delta.values)\n",
        "distances, indices = nbrs.kneighbors(delta.values)\n",
        "\n",
        "# The outlier score is set as the distance between the point and its k-th nearest neighbour\n",
        "outlier_score = distances[:,knn-1]\n",
        "\n",
        "# Plot 3D scatterplot of outlier scores\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "p = ax.scatter(delta.MSFT,delta.F,delta.BAC,c=outlier_score,cmap='jet')\n",
        "ax.set_xlabel('Microsoft')\n",
        "ax.set_ylabel('Ford')\n",
        "ax.set_zlabel('Bank of America')\n",
        "fig.colorbar(p)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiJazbQwf3qi"
      },
      "source": [
        "The results are slightly different than the one shown in Section 1 due to the difference distance used (Euclidean distance vs Mahalanobis distance) and the proximity criterion to detect the outliers. \n",
        "\n",
        "We can examine the dates associated with the top-5 highest outlier scores as follows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SmpjAtDf3qj"
      },
      "source": [
        "outlier = pd.DataFrame(outlier_score, index=delta.index, columns=['Outlier score'])\n",
        "result = pd.concat((delta,outlier), axis=1)\n",
        "result.nlargest(5,'Outlier score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jblKfsoPf3qj"
      },
      "source": [
        "Finally, similarly to what was carried out in Section 1, we can inspect stocks around those outlier dates for each company. The below figure inspects the delta values for each company around the date of the 3rd detected outlier, on 7th October 2008, which represents a key date for the 2008 financial resession with large drops in stock values. Two companies seem to be primarily responsible for the creation of this outlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXQddWQsf3qk"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ts = delta[440:447]\n",
        "ts.plot.line(ax=ax)\n",
        "ax.set_xticks(range(7))\n",
        "ax.set_xticklabels(ts.index)\n",
        "ax.set_ylabel('Percent Change')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OUWbL6pf3ql"
      },
      "source": [
        "## 3. Outlier detection using classification-based methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9zLPSof3ql"
      },
      "source": [
        "The **support vector machine (SVM)** algorithm developed initially for binary classification can be used for one-class classification, and therefore for outlier detection using a classification-based method where we construct a classifier to describe only the normal class.\n",
        "\n",
        "When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as **One-Class SVM**.\n",
        "\n",
        "In this section, we will work with a different dataset on house prices available at: https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv . This dataset has 13 input variables that describe the properties of the house and suburb and requires the prediction of the median value of houses in the suburb in thousands of dollars. Information and metadata about the dataset can be found at: https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names . Please spend some time to inspect the dataset and its metadata.\n",
        "\n",
        "As a first step, we load the dataset and split it into input and output (target) elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPolDwjqf3qm"
      },
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "# Loading the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(url, header=None)\n",
        "\n",
        "# Extracting the values from the dataframe\n",
        "data = df.values\n",
        "\n",
        "# Split dataset into input and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "# Summarize the shape of the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmC-8tZ0f3qm"
      },
      "source": [
        "Using the OneClassSVM() function in scikit-learn, we can initialise and train an one-class SVM classifier on the input data. Please study the [OneClassSVM() documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html) for information on input arguments. \n",
        "\n",
        "We can then print the estimated labels, which for each object are -1 for outliers and 1 for inliers (i.e. data points that are considered normal)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0G6qtG4f3qm"
      },
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "ee = OneClassSVM(nu=0.01,gamma='auto')\n",
        "yhat = ee.fit_predict(X) # Perform fit on input data and returns labels for that input data.\n",
        "\n",
        "print(yhat) # Print labels: -1 for outliers and 1 for inliers."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8kF0dfSf3qn"
      },
      "source": [
        "Having trained the one-class SVM, we can then select all rows from the dataset that are **not** outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6alcrYCYf3qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e184b45-4a01-4990-a421-2d7128724c3a"
      },
      "source": [
        "# Select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "X, y = X[mask, :], y[mask]\n",
        "\n",
        "# Summarize the shape of the updated dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(190, 13) (190,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phamfB1Df3qn"
      },
      "source": [
        "As we see, the new dataset has a significantly smaller number of objects, all of which are considered by the one-class SVM to be `inliers', i.e. to belong to the normal distribution of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "DBSCAN is a non-parametric, density-based outlier detection technique used for one- or multi- dimensional feature space. This technique is based on the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering method. In DBSCAN, there are three classes of points which are given as follows :\n",
        "\n",
        "- Core Points: data points that have at least MinPts neighbouring data points within a distance ℇ. </br>\n",
        "\n",
        "- Border Points: neighbours of a Core Point within the distance ℇ but with less than MinPts neighbours within the distance ℇ. </br>\n",
        "\n",
        "- Outliers: all other data points, these points do not lie in any cluster.\n",
        "\n",
        "DBSCAN requires some of the below parameters :\n",
        "\n",
        "- Epsilon (ℇ): eps which specifies how should be the distance between each point to be considered a part of a cluster. This is the most important parameter to choose appropriately for your data set and distance function. The default value is 0.5. </br>\n",
        "\n",
        "- Min points (MinPts): min_sample which specifies how many neighbours a point should have to be included in a cluster. The default value is 5.\n",
        "\n",
        "- Metric : metric to use when calculating the distance between instances in a feature array. The default metric is ‘euclidean’.\n",
        "\n",
        "Here, a data instance is considered as an outlier, if it does not belong to any cluster. \n",
        "\n",
        "DBSCAN is easy to implement and is available in python as sklearn.cluster.DBSCAN. In this method, Outliers are labelled as -1.\n",
        "\n",
        "For this exercise you will be using the Weight and Height Dataset from Kaggle (available in the lab supplementary material). "
      ],
      "metadata": {
        "id": "1w78zgkAbkoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the data\n",
        "df = pd.read_csv(\"weight-height.csv\")\n",
        "print(f\"Shape of data : {df.shape}\\n\")\n",
        "print(f\"Sample of data : \\n{df.head()}\")"
      ],
      "metadata": {
        "id": "jToW-Ywbc56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) At first create the scatterplot between the variables ‘Height’ and ‘Weight’ and visualize their relationship. \n",
        "\n",
        "2) Then apply the DBSCAN method for the height feature. For this the below steps will be needed: </br>\n",
        "- import DBSCAN library\n",
        "\n",
        "- convert Height column to array with shape (-1,1) \n",
        "\n",
        "- create DBSCAN model with parameters eps=0.5 and min_samples=8\n",
        "\n",
        "- fit model to data\n",
        "\n",
        "- print total number of outliers\n",
        "\n",
        "- print the outliers "
      ],
      "metadata": {
        "id": "lYzMyWs_dJjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import DBSCAN library\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Convert Height column to array with shape (-1,1) \n",
        "data = df[\"Height\"].values.reshape(-1,1)\n",
        "\n",
        "print(f\"Shape of data : {data.shape}\")\n",
        "print(f\"Type of data : {type(data)}\")\n",
        "\n",
        "# Create DBSCAN model with parameters\n",
        "model = DBSCAN(eps=0.5,min_samples=8)\n",
        "\n",
        "# Fit model to data\n",
        "model.fit(data)\n",
        "\n",
        "# Printing total number of outliers\n",
        "print(f\"Total number of outliers : {sum(model.labels_ == -1)}\")\n",
        "\n",
        "# Outliers \n",
        "outliers = df[\"Height\"][model.labels_ == -1]\n",
        "print(f\"Outliers in Height column : \\n{outliers}\")"
      ],
      "metadata": {
        "id": "wgwgcfq7dZgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}